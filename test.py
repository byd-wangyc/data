```python
import os
import csv
import numpy as np
import torch
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
from sklearn.metrics import roc_auc_score, accuracy_score
from scipy.stats import spearmanr

# ------------------------------
# 1. é…ç½®å‚æ•°
# ------------------------------
model_path = "./bge-m3"          # é¢„è®­ç»ƒæ¨¡å‹ï¼ˆæœ¬åœ°è·¯å¾„ï¼‰
train_path = "./data/train.csv"  # è®­ç»ƒé›†
test_path = "./data/test.csv"    # æµ‹è¯•é›†
output_dir = "./output_model"    # è¾“å‡ºå¾®è°ƒåæ¨¡å‹è·¯å¾„
batch_size = 32
epochs = 3
lr = 2e-5
warmup_ratio = 0.1

# ------------------------------
# 2. åŠ è½½æ¨¡å‹
# ------------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
model = SentenceTransformer(model_path, device=device)

# ------------------------------
# 3. æ•°æ®åŠ è½½å‡½æ•°
# ------------------------------
def load_dataset(csv_path):
    examples = []
    with open(csv_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            text1, text2, label = row["text1"], row["text2"], float(row["label"])
            examples.append(InputExample(texts=[text1, text2], label=label))
    return examples

train_examples = load_dataset(train_path)
test_examples = load_dataset(test_path)

# ------------------------------
# 4. æ„å»º DataLoader
# ------------------------------
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)
train_loss = losses.CosineSimilarityLoss(model)

# ------------------------------
# 5. å®šä¹‰ warmup
# ------------------------------
warmup_steps = int(len(train_dataloader) * epochs * warmup_ratio)

# ------------------------------
# 6. å®šä¹‰è¯„ä¼°å‡½æ•°
# ------------------------------
def evaluate(model, examples):
    """è®¡ç®—å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦ä¸çœŸå®æ ‡ç­¾çš„ç›¸å…³æ€§"""
    texts1 = [ex.texts[0] for ex in examples]
    texts2 = [ex.texts[1] for ex in examples]
    labels = np.array([ex.label for ex in examples])

    emb1 = model.encode(texts1, normalize_embeddings=True, batch_size=64)
    emb2 = model.encode(texts2, normalize_embeddings=True, batch_size=64)
    sims = np.sum(emb1 * emb2, axis=1)  # ä½™å¼¦ç›¸ä¼¼åº¦

    corr, _ = spearmanr(labels, sims)  # è®¡ç®—æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°
    auc = roc_auc_score(labels, sims) if len(set(labels)) > 1 else None

    # ç”¨ 0.5 é˜ˆå€¼ç²—ç•¥è®¡ç®—å‡†ç¡®ç‡
    acc = accuracy_score(labels, (sims > 0.5).astype(int))
    return {"spearman": corr, "auc": auc, "accuracy": acc}

# ------------------------------
# 7. å¾®è°ƒå‰çš„è¯„ä¼°
# ------------------------------
print("ğŸ” å¾®è°ƒå‰æ¨¡å‹è¯„ä¼°ä¸­...")
before_metrics = evaluate(model, test_examples)
print(f"å¾®è°ƒå‰ - Spearman: {before_metrics['spearman']:.4f}, "
      f"AUC: {before_metrics['auc']:.4f}, ACC: {before_metrics['accuracy']:.4f}")

# ------------------------------
# 8. å¼€å§‹å¾®è°ƒ
# ------------------------------
print("ğŸš€ å¼€å§‹å¾®è°ƒ...")
model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=epochs,
    warmup_steps=warmup_steps,
    optimizer_params={'lr': lr},
    output_path=output_dir,
    show_progress_bar=True
)

# ------------------------------
# 9. åŠ è½½å¾®è°ƒåæ¨¡å‹å†è¯„ä¼°
# ------------------------------
model_finetuned = SentenceTransformer(output_dir, device=device)
print("ğŸ” å¾®è°ƒåæ¨¡å‹è¯„ä¼°ä¸­...")
after_metrics = evaluate(model_finetuned, test_examples)

print(f"âœ… å¾®è°ƒå - Spearman: {after_metrics['spearman']:.4f}, "
      f"AUC: {after_metrics['auc']:.4f}, ACC: {after_metrics['accuracy']:.4f}")

# ------------------------------
# 10. å¯¹æ¯”ç»“æœ
# ------------------------------
print("\nğŸ“Š å¾®è°ƒæ•ˆæœå¯¹æ¯”ï¼š")
print(f"Spearman ç›¸å…³æå‡: {after_metrics['spearman'] - before_metrics['spearman']:.4f}")
print(f"AUC æå‡: {after_metrics['auc'] - before_metrics['auc']:.4f}")
print(f"å‡†ç¡®ç‡æå‡: {after_metrics['accuracy'] - before_metrics['accuracy']:.4f}")
```
